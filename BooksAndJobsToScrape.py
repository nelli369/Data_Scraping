# -*- coding: utf-8 -*-
"""hmk3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ndnvGSmY2TN4V2OWD1SnpGs3iEV67pmM
"""

import time
import requests
import numpy as np
import pandas as pd
from scrapy.http import TextResponse
import re

"""Problem 1"""

def books_scraper(url,base_url="http://books.toscrape.com/"):
    page = requests.get(url)
    response = TextResponse(body=page.text,url=url,encoding="utf-8") 
    prices = response.css("p.price_color::text").extract()
    price = [float(i.replace("Â£", "")) for i in prices]
    book_url = response.css("h3 >a::attr(href)").extract()
    pic_url = response.css("img::attr(src)").extract()
    star = response.css("p[class^='star-rating']::attr(class)").extract()
    star_rating = []
    for i in star:
        star_rating.append(i.replace("star-rating", ""))   
    base_url = "http://books.toscrape.com/catalogue/"
    bookurl = [base_url + i for i in book_url]
    picurl = [base_url + i for i in pic_url]
    genere = []
    desc = []
    for i in bookurl:
        page = requests.get(i)
        response = TextResponse(body=page.text,url=i,encoding="utf-8")
        genere.append(response.css("li~li~li > a::text")[0].extract())
        desc.append(response.css("article[class='product_page'] > p::text").extract_first())
    
    return pd.DataFrame({  "Price":price, "BooksHyperlinks":bookurl, "PicsHyperlinks":picurl,"Star_Ratings":star_rating,'Generes':genere,"AboutBook":desc})

books = []
for i in range(1,100):
    cureent_page =books_scraper(url = f"http://books.toscrape.com/catalogue/page-{i}.html")
    if cureent_page.shape[0] == 0:
        break
    else:
        books.append(cureent_page)

books = pd.concat(books)
books

books.to_csv('books.csv', index=False)

books.to_csv('books.csv', index=False)

books["Price"].max()

books[books['Price'] == books['Price'].max()]['Generes']

books['Star_Ratings'].value_counts()

rating_map = {
    'Two': 2,
    'One': 1,
    'Five': 5,
    'Four': 4,
    'Three': 3
}

books['Star_Ratings'] = books['Star_Ratings'].apply(lambda x: x.strip())

books['Star_Ratings'] =books['Star_Ratings'].replace(rating_map)

books.corr()

books.groupby('Star_Ratings').mean()['Price']

"""Problem 2"""

def jobs_scraper(url):
    page = requests.get(url)
    response = TextResponse(body=page.text,url=url,encoding="utf-8")
    companies_name = response.xpath("//p[@class='job_list_company_title']/text()").extract()
    vac_name = response.xpath("//p[@class='font_bold']/text()").extract()
    base_url = "https://staff.am"
    urls = response.xpath("//div[@class='web_item_card hs_job_list_item']/a/@href").extract()
    vacs_url = [base_url + i for i in urls]
    deadline1 = response.css("div[class = 'job-inner job-list-deadline'] >p:not([class='job_location'])")
    deadline2 =[i.css('::text').extract()[1] for i in deadline1]
    deadline = [i.replace('\n'," ") for i in deadline2]
    location1 = response.css("div[class = 'job-inner job-list-deadline'] >p[class='job_location']")
    location2 = [i.css("::text").extract()[1] for i in location1]
    location = [i.replace('\n',"").strip() for i in location2]
    return pd.DataFrame({"Companies":companies_name,"Vacancies":vac_name,'Links':vacs_url,"Deadline":deadline,'Location':location})

jobs = []
for i in range(1,100):
    cureent_page =jobs_scraper(url = f"https://staff.am/en/jobs?page={i}&per-page=50")
    if cureent_page.shape[0] == 0:
        break
    else:
        jobs.append(cureent_page)

jobs= pd.concat(jobs)
jobs

jobs['Companies'].value_counts()

jobs[jobs['Vacancies'].str.contains("Data")]

jobs[jobs['Vacancies'].str.contains("Data")]['Vacancies'].count()

jobs.to_csv('jobs.csv', index=False)

