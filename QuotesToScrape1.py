# -*- coding: utf-8 -*-
"""DS hmk2 pr1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1V6NO0o82WlhCq3LD2TUXfCsYL6PHp3GF
"""

import time
import requests
import numpy as np
import pandas as pd
from scrapy.http import TextResponse

import re

def get_request(url):
    page = requests.get(url)
    response = TextResponse(body=page.text,url=url,encoding="utf-8")
    return response

def quote_scraper(response):
    quotes = response.css("div.quote > span.text::text").extract()
    author = response.css("small.author::text").extract()
    tags_div =response.css("div.tags ")
    tags = [i.css("a.tag::text").extract() for i in tags_div]
    hyper = response.css("small.author ~ a::attr(href)").extract()
    base_link = 'http://quotes.toscrape.com'
    hyperlink = [base_link + i for i in hyper]
    return pd.DataFrame({"Quotes":quotes,"Authors":author,"Tags":tags,"Hyperlinks":hyperlink})

pages = []
url = "http://quotes.toscrape.com/"

while True:
    response = get_request(url)
    pages.append(quote_scraper(response))
    next_page_url = response.css("li.next > a::attr(href)").extract_first()
    if next_page_url:
        url = response.urljoin(next_page_url)
    else:
        break

pg = pd.concat(pages)
pg

